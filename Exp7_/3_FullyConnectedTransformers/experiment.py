import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
file_path = r'Exp7_\data\MSFT_Monthly_stock_prizes.csv'
data = pd.read_csv(file_path)

# –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º —Å—Ç–æ–ª–±—Ü—ã
data.columns = ["Date", "Open", "High", "Low", "Close", "Volume"]

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º "Date" –≤ datetime –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
data["Date"] = pd.to_datetime(data["Date"])
data = data.sort_values("Date")

# –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
features = ["Open", "High", "Low", "Volume"]
target = "Close"

# –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
test_sizes = [0.1, 0.2, 0.5, 0.8, 0.9]
num_layers_list = [1, 2, 4, 6, 8, 10]  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
sequence_length = 10  # –î–ª–∏–Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –æ–∫–Ω–∞

# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
scaler = StandardScaler()

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
def create_sequences(X, y, sequence_length):
    X_seq, y_seq = [], []
    for i in range(len(X) - sequence_length):
        X_seq.append(X[i:i+sequence_length])
        y_seq.append(y.iloc[i+sequence_length])
    return np.array(X_seq), np.array(y_seq)

# –§—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ —Å —Ä–∞–∑–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–ª–æ–µ–≤
def build_transformer_model(input_shape, num_layers, head_size=64, num_heads=2, ff_dim=128, dropout=0.1):
    inputs = keras.Input(shape=input_shape)
    x = inputs

    # –î–æ–±–∞–≤–ª—è–µ–º num_layers —Å–ª–æ–µ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
    for _ in range(num_layers):
        x = layers.LayerNormalization(epsilon=1e-6)(x)
        x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)
        x = layers.Dropout(dropout)(x)
        res = x + inputs  # Skip connection

        x = layers.LayerNormalization(epsilon=1e-6)(res)
        x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation="relu")(x)
        x = layers.Dropout(dropout)(x)
        x = layers.Conv1D(filters=input_shape[-1], kernel_size=1)(x)
        x = x + res  # Skip connection

    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(64, activation="relu")(x)
    outputs = layers.Dense(1)(x)

    model = keras.Model(inputs, outputs)
    model.compile(optimizer="adam", loss="mean_squared_error")

    return model

results = []

# –ó–∞–ø—É—Å–∫–∞–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
for test_size in test_sizes:
    print(f"\nüîπ –ó–∞–ø—É—Å–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ —Å test_size={test_size}...")

    # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ train/test –±–µ–∑ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏—è
    train_data, test_data = train_test_split(data, test_size=test_size, shuffle=False)

    X_train, y_train = train_data[features], train_data[target]
    X_test, y_test = test_data[features], test_data[target]

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    print(f"Train size: {X_train.shape[0]}, Test size: {X_test.shape[0]}, Sequence length: {sequence_length}")

    if X_train.shape[0] <= sequence_length or X_test.shape[0] <= sequence_length:
        print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º test_size={test_size}, —Ç–∞–∫ –∫–∞–∫ –¥–∞–Ω–Ω—ã—Ö –º–µ–Ω—å—à–µ sequence_length.")
        continue

    # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train, sequence_length)
    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test, sequence_length)

    # –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–µ
    for num_layers in num_layers_list:
        print(f"\nüü¢ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å {num_layers} —Å–ª–æ—è–º–∏...")

        model = build_transformer_model(input_shape=(sequence_length, len(features)), num_layers=num_layers)
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        model.fit(X_train_seq, y_train_seq, epochs=50, batch_size=32, verbose=1)

        # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º
        print(f"üîµ –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è {num_layers} —Å–ª–æ–µ–≤...")
        y_pred = model.predict(X_test_seq)

        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏: MAE –∏ –¥–∏—Å–ø–µ—Ä—Å–∏—é –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        mae = mean_absolute_error(y_test_seq, y_pred)
        variance = np.var(y_pred)

        results.append({
            "test_size": test_size,
            "num_layers": num_layers,
            "mae": mae,
            "variance": variance
        })

        print(f"‚úÖ Test size: {test_size}, Layers: {num_layers}, MAE: {mae:.4f}, Variance: {variance:.4f}")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
results_df = pd.DataFrame(results)
results_df.to_csv("transformer_results.csv", index=False)
print("üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'transformer_results.csv'")
